{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSI 4107, Winter 2021\n",
    "## Assignment 1 - Microblog information retrieval system\n",
    "\n",
    "### Step 1: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas_read_xml as pdx\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "      <th>title</th>\n",
       "      <th>querytime</th>\n",
       "      <th>querytweettime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Number: MB001</td>\n",
       "      <td>BBC World Service staff cuts</td>\n",
       "      <td>Tue Feb 08 12:30:27 +0000 2011</td>\n",
       "      <td>34952194402811904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Number: MB002</td>\n",
       "      <td>2022 FIFA soccer</td>\n",
       "      <td>Tue Feb 08 18:51:44 +0000 2011</td>\n",
       "      <td>35048150574039040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Number: MB003</td>\n",
       "      <td>Haiti Aristide return</td>\n",
       "      <td>Tue Feb 08 21:32:13 +0000 2011</td>\n",
       "      <td>35088534306033665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Number: MB004</td>\n",
       "      <td>Mexico drug war</td>\n",
       "      <td>Wed Feb 02 17:22:14 +0000 2011</td>\n",
       "      <td>32851298193768448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number: MB005</td>\n",
       "      <td>NIST computer security</td>\n",
       "      <td>Fri Feb 04 17:44:09 +0000 2011</td>\n",
       "      <td>33581589627666432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             num                         title  \\\n",
       "0  Number: MB001  BBC World Service staff cuts   \n",
       "1  Number: MB002              2022 FIFA soccer   \n",
       "2  Number: MB003         Haiti Aristide return   \n",
       "3  Number: MB004               Mexico drug war   \n",
       "4  Number: MB005        NIST computer security   \n",
       "\n",
       "                        querytime     querytweettime  \n",
       "0  Tue Feb 08 12:30:27 +0000 2011  34952194402811904  \n",
       "1  Tue Feb 08 18:51:44 +0000 2011  35048150574039040  \n",
       "2  Tue Feb 08 21:32:13 +0000 2011  35088534306033665  \n",
       "3  Wed Feb 02 17:22:14 +0000 2011  32851298193768448  \n",
       "4  Fri Feb 04 17:44:09 +0000 2011  33581589627666432  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queriesDf = pdx.read_xml(\"queries.xml\", ['queries', 'top'])\n",
    "queriesDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿34952194402811904</td>\n",
       "      <td>Save BBC World Service from Savage Cuts http:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34952186328784896</td>\n",
       "      <td>a lot of people always make fun about the end ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34952041415581696</td>\n",
       "      <td>ReThink Group positive in outlook: Technology ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34952018120409088</td>\n",
       "      <td>'Zombie' fund manager Phoenix appoints new CEO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34952008683229185</td>\n",
       "      <td>Latest:: Top World Releases http://globalclass...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                               text\n",
       "0  ﻿34952194402811904  Save BBC World Service from Savage Cuts http:/...\n",
       "1   34952186328784896  a lot of people always make fun about the end ...\n",
       "2   34952041415581696  ReThink Group positive in outlook: Technology ...\n",
       "3   34952018120409088  'Zombie' fund manager Phoenix appoints new CEO...\n",
       "4   34952008683229185  Latest:: Top World Releases http://globalclass..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read tweets and transform into dataframe\n",
    "data = []\n",
    "with open('Trec_microblog11.txt', 'r', encoding='utf-8', errors='replace') as infile:\n",
    "    lines = infile.readlines()\n",
    "    for i in lines:\n",
    "        data.append(i.split('\\t'))\n",
    "        \n",
    "data = np.array(data) #2d numpy array\n",
    "        \n",
    "df = pd.DataFrame({'ID': data[:, 0], 'text': data[:, 1]})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-d442573762f1>:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['text_without_stopwords'] = df['text_without_stopwords'].str.replace('\\d+', '')\n",
      "<ipython-input-5-d442573762f1>:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  queriesDf['text_without_stopwords'] = queriesDf['text_without_stopwords'].str.replace('\\d+', '')\n"
     ]
    }
   ],
   "source": [
    "df['text_without_stopwords'] = df['text'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in (stop)]))\n",
    "df['text_without_stopwords'] = df['text_without_stopwords'].str.replace('\\d+', '')\n",
    "df['text_without_stopwords']=df['text_without_stopwords'].apply(str.lower)\n",
    "\n",
    "queriesDf['text_without_stopwords'] = queriesDf['title'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in (stop)]))\n",
    "queriesDf['text_without_stopwords'] = queriesDf['text_without_stopwords'].str.replace('\\d+', '')\n",
    "queriesDf['text_without_stopwords'] = queriesDf['text_without_stopwords'].apply(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "queriesDf['topic_id'] = queriesDf['num'].apply(lambda x : x.split(' '))\n",
    "queriesDf['topic_id'] = queriesDf['topic_id'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "pstemmer = PorterStemmer()\n",
    "\n",
    "#tokenize and stem a string\n",
    "#not used cuz got worse results with stemmed words\n",
    "def tokenizer_stemmer(str):\n",
    "    tokens = tokenizer.tokenize(str)\n",
    "    for i in range(len(tokens)):\n",
    "        tokens[i] = pstemmer.stem(tokens[i])\n",
    "    return tokens\n",
    "\n",
    "#tokenize dataframes\n",
    "df['token'] = df['text_without_stopwords'].apply(lambda x: tokenizer.tokenize(x))\n",
    "queriesDf['token'] = queriesDf['text_without_stopwords'].apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# detokenize for easier query/document vectorization\n",
    "df['detoken']=df['token'].apply(lambda x: TreebankWordDetokenizer().detokenize(x))\n",
    "queriesDf['detoken']=queriesDf['token'].apply(lambda x: TreebankWordDetokenizer().detokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "      <th>title</th>\n",
       "      <th>querytime</th>\n",
       "      <th>querytweettime</th>\n",
       "      <th>text_without_stopwords</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>token</th>\n",
       "      <th>detoken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Number: MB001</td>\n",
       "      <td>BBC World Service staff cuts</td>\n",
       "      <td>Tue Feb 08 12:30:27 +0000 2011</td>\n",
       "      <td>34952194402811904</td>\n",
       "      <td>bbc world service staff cuts</td>\n",
       "      <td>MB001</td>\n",
       "      <td>[bbc, world, service, staff, cuts]</td>\n",
       "      <td>bbc world service staff cuts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Number: MB002</td>\n",
       "      <td>2022 FIFA soccer</td>\n",
       "      <td>Tue Feb 08 18:51:44 +0000 2011</td>\n",
       "      <td>35048150574039040</td>\n",
       "      <td>fifa soccer</td>\n",
       "      <td>MB002</td>\n",
       "      <td>[fifa, soccer]</td>\n",
       "      <td>fifa soccer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Number: MB003</td>\n",
       "      <td>Haiti Aristide return</td>\n",
       "      <td>Tue Feb 08 21:32:13 +0000 2011</td>\n",
       "      <td>35088534306033665</td>\n",
       "      <td>haiti aristide return</td>\n",
       "      <td>MB003</td>\n",
       "      <td>[haiti, aristide, return]</td>\n",
       "      <td>haiti aristide return</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Number: MB004</td>\n",
       "      <td>Mexico drug war</td>\n",
       "      <td>Wed Feb 02 17:22:14 +0000 2011</td>\n",
       "      <td>32851298193768448</td>\n",
       "      <td>mexico drug war</td>\n",
       "      <td>MB004</td>\n",
       "      <td>[mexico, drug, war]</td>\n",
       "      <td>mexico drug war</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number: MB005</td>\n",
       "      <td>NIST computer security</td>\n",
       "      <td>Fri Feb 04 17:44:09 +0000 2011</td>\n",
       "      <td>33581589627666432</td>\n",
       "      <td>nist computer security</td>\n",
       "      <td>MB005</td>\n",
       "      <td>[nist, computer, security]</td>\n",
       "      <td>nist computer security</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             num                         title  \\\n",
       "0  Number: MB001  BBC World Service staff cuts   \n",
       "1  Number: MB002              2022 FIFA soccer   \n",
       "2  Number: MB003         Haiti Aristide return   \n",
       "3  Number: MB004               Mexico drug war   \n",
       "4  Number: MB005        NIST computer security   \n",
       "\n",
       "                        querytime     querytweettime  \\\n",
       "0  Tue Feb 08 12:30:27 +0000 2011  34952194402811904   \n",
       "1  Tue Feb 08 18:51:44 +0000 2011  35048150574039040   \n",
       "2  Tue Feb 08 21:32:13 +0000 2011  35088534306033665   \n",
       "3  Wed Feb 02 17:22:14 +0000 2011  32851298193768448   \n",
       "4  Fri Feb 04 17:44:09 +0000 2011  33581589627666432   \n",
       "\n",
       "         text_without_stopwords topic_id                               token  \\\n",
       "0  bbc world service staff cuts    MB001  [bbc, world, service, staff, cuts]   \n",
       "1                   fifa soccer    MB002                      [fifa, soccer]   \n",
       "2         haiti aristide return    MB003           [haiti, aristide, return]   \n",
       "3               mexico drug war    MB004                 [mexico, drug, war]   \n",
       "4        nist computer security    MB005          [nist, computer, security]   \n",
       "\n",
       "                        detoken  \n",
       "0  bbc world service staff cuts  \n",
       "1                   fifa soccer  \n",
       "2         haiti aristide return  \n",
       "3               mexico drug war  \n",
       "4        nist computer security  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queriesDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>text_without_stopwords</th>\n",
       "      <th>token</th>\n",
       "      <th>detoken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿34952194402811904</td>\n",
       "      <td>Save BBC World Service from Savage Cuts http:/...</td>\n",
       "      <td>save bbc world service savage cuts http://www....</td>\n",
       "      <td>[save, bbc, world, service, savage, cuts, http...</td>\n",
       "      <td>save bbc world service savage cuts http www pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34952186328784896</td>\n",
       "      <td>a lot of people always make fun about the end ...</td>\n",
       "      <td>lot people always make fun end world question ...</td>\n",
       "      <td>[lot, people, always, make, fun, end, world, q...</td>\n",
       "      <td>lot people always make fun end world question ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34952041415581696</td>\n",
       "      <td>ReThink Group positive in outlook: Technology ...</td>\n",
       "      <td>rethink group positive outlook: technology sta...</td>\n",
       "      <td>[rethink, group, positive, outlook, technology...</td>\n",
       "      <td>rethink group positive outlook technology staf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34952018120409088</td>\n",
       "      <td>'Zombie' fund manager Phoenix appoints new CEO...</td>\n",
       "      <td>'zombie' fund manager phoenix appoints new ceo...</td>\n",
       "      <td>[zombie, fund, manager, phoenix, appoints, new...</td>\n",
       "      <td>zombie fund manager phoenix appoints new ceo p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34952008683229185</td>\n",
       "      <td>Latest:: Top World Releases http://globalclass...</td>\n",
       "      <td>latest:: top world releases http://globalclass...</td>\n",
       "      <td>[latest, top, world, releases, http, globalcla...</td>\n",
       "      <td>latest top world releases http globalclassifie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                               text  \\\n",
       "0  ﻿34952194402811904  Save BBC World Service from Savage Cuts http:/...   \n",
       "1   34952186328784896  a lot of people always make fun about the end ...   \n",
       "2   34952041415581696  ReThink Group positive in outlook: Technology ...   \n",
       "3   34952018120409088  'Zombie' fund manager Phoenix appoints new CEO...   \n",
       "4   34952008683229185  Latest:: Top World Releases http://globalclass...   \n",
       "\n",
       "                              text_without_stopwords  \\\n",
       "0  save bbc world service savage cuts http://www....   \n",
       "1  lot people always make fun end world question ...   \n",
       "2  rethink group positive outlook: technology sta...   \n",
       "3  'zombie' fund manager phoenix appoints new ceo...   \n",
       "4  latest:: top world releases http://globalclass...   \n",
       "\n",
       "                                               token  \\\n",
       "0  [save, bbc, world, service, savage, cuts, http...   \n",
       "1  [lot, people, always, make, fun, end, world, q...   \n",
       "2  [rethink, group, positive, outlook, technology...   \n",
       "3  [zombie, fund, manager, phoenix, appoints, new...   \n",
       "4  [latest, top, world, releases, http, globalcla...   \n",
       "\n",
       "                                             detoken  \n",
       "0  save bbc world service savage cuts http www pe...  \n",
       "1  lot people always make fun end world question ...  \n",
       "2  rethink group positive outlook technology staf...  \n",
       "3  zombie fund manager phoenix appoints new ceo p...  \n",
       "4  latest top world releases http globalclassifie...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df['token'][26]#test to see if numbers have been removed\n",
    "#print(df['token'][12898])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# key [doc]\n",
    "# reverse index\n",
    "def create_index (data):\n",
    "        index = defaultdict(list)\n",
    "        count=0\n",
    "        for i, tokens in enumerate(data):\n",
    "            for token in tokens:\n",
    "                index[token].append(i)\n",
    "                #index[save].append(doc)\n",
    "        return index\n",
    "\n",
    "\n",
    "\n",
    "def count_occurrences(arr):\n",
    "    return list(Counter(arr).items())\n",
    "\n",
    "\n",
    "\n",
    "# key, [(doc#, count)]\n",
    "def final_index(index):\n",
    "    final=defaultdict(list)\n",
    "    for key in index.keys():\n",
    "        final[key] = count_occurrences(index[key])\n",
    "    return final\n",
    "\n",
    "# each token (save, bbc) as your dict key\n",
    "# append the count_occurrences based on index\n",
    "#final_index creates the final hash table for inverted index. {word: [(doc#, number of occurrences in that document)]}\n",
    "# doc# in this case is the line number of the table, not the message ID.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=create_index(df['token'])\n",
    "test2=count_occurrences([0, 1, 44, 44, 443])\n",
    "rev_index=final_index(test)\n",
    "\n",
    "#print(len(rev_index.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['save', 'bbc', 'world', 'service', 'savage', 'cuts', 'http', 'www', 'petitionbuzz', 'com', 'petitions', 'savews', 'lot', 'people', 'always', 'make', 'fun', 'end', 'question', 'is', 'are', 'u', 'ready', 'for', 'it', 'rethink', 'group', 'positive', 'outlook', 'technology', 'staffing', 'specialist', 'expects', 'revenues', 'marg', 'bit', 'ly', 'hfjtmy', 'zombie', 'fund', 'manager', 'phoenix', 'appoints', 'new', 'ceo', 'buys', 'funds', 'closed', 'business', 'dxrlh', 'latest', 'top', 'releases', 'globalclassified', 'net', 'cdt', 'presents', 'alice', 'in', 'wonderland', 'catonsville', 'dinner', 'posted', 'the', 'fb', 'me', 'gmicayt', 'territory', 'location', 'calgary', 'alberta', 'canada', 'job', 'category', 'bu', 'eomt', 'jobs', 'news', 'today', 'free', 'school', 'funding', 'plans', 'lack', 'transparency', 'co', 'uk', 'hi', 'newsid_', 'stm', 'manchester', 'city', 'council', 'details', 'saving', 'plan', 'fypypc', 'depressing', 'apparently', 'we']\n"
     ]
    }
   ],
   "source": [
    "print(list(rev_index.keys())[0:100]) # get a sample of 100 tokens from vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Retrieval and Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87685\n"
     ]
    }
   ],
   "source": [
    "# query: list of tokens for a query\n",
    "# docs: inverted hash index with key:value of token:[(doc_num,num_occurances,...)]\n",
    "def related_documents_to_query(query, docs):\n",
    "    related_docs = []\n",
    "    for token in query:\n",
    "        if token in docs:\n",
    "            related_docs+=docs[token]\n",
    "\n",
    "    nums_only = []\n",
    "    for x in related_docs:\n",
    "        nums_only.append(x[0])\n",
    "\n",
    "    return set(nums_only)\n",
    "\n",
    "\n",
    "#get vocabulary\n",
    "vocab= []\n",
    "for arr in df['token']:\n",
    "    vocab += arr\n",
    "vocab = set(vocab)\n",
    "\n",
    "#create td-idf matrix\n",
    "corpus = df['detoken']\n",
    "docVectorizer = TfidfVectorizer(vocabulary = vocab, sublinear_tf=True)\n",
    "docMatrix = docVectorizer.fit_transform(corpus)\n",
    "docVocabulary = docVectorizer.vocabulary_\n",
    "#docMatrix.toarray()\n",
    "\n",
    "\n",
    "#find cosine similarity\n",
    "#returns an array of (doc_num, similarity) tuples\n",
    "def similarity_cosine(query_vector, docs):\n",
    "    sim_arr = []\n",
    "    \n",
    "    for doc in docs: \n",
    "        similarity = pairwise.cosine_similarity(doc[1], query_vector)\n",
    "        sim_arr.append((doc[0], similarity[0][0]))\n",
    "    return sim_arr \n",
    "\n",
    "\n",
    "print(len(docVocabulary)) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# final results: array of length # of queries with tuples of (query, top_1000_docs)\n",
    "# where top_1000_docs is an array of tuples of (doc_num, similarity)\n",
    "final_results = []\n",
    "for index, q_row in queriesDf.iterrows():\n",
    "    # for each query, get set of related docs (doc #, num of occurences)\n",
    "    related_docs = related_documents_to_query(q_row['token'], rev_index)\n",
    "    \n",
    "    related_docs_matrix = []\n",
    "    #get vectors of related docs\n",
    "    for doc_num in related_docs:\n",
    "        related_docs_matrix.append((doc_num, docMatrix[doc_num]) )\n",
    "        \n",
    "    # get similarities\n",
    "    qvec = docVectorizer.transform([q_row['detoken']]) # vectorize query\n",
    "    sim_arr = similarity_cosine(qvec, related_docs_matrix)\n",
    "    \n",
    "    #sort by similarity\n",
    "    ranked = sorted(sim_arr, key = lambda x : x[1], reverse = True)\n",
    "    \n",
    "    # rank top 1000\n",
    "    top_1000 = ranked[0:1000]\n",
    "    \n",
    "    # add to final results array\n",
    "    final_results.append( (q_row['num'], top_1000) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Write to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample line: topic_id Q0 docno rank score tag\n",
    "# topic_id: queriesDf['topic_id'][i]\n",
    "# docno: df['ID'][resultsMatrix[i][j][0]]\n",
    "# rank: j from resultsMatrix[i], +1 because highest rank is 1 not 0\n",
    "# score: resultsMatrix[i][j][1]\n",
    "# tag: passed in at function call\n",
    "\n",
    "def write_results(tag, resultMatrix):\n",
    "    \n",
    "    f = open(\"results.txt\", \"w+\", encoding=\"utf-8\")\n",
    "    \n",
    "    for i in range(len(resultMatrix)):\n",
    "        topic_id = queriesDf['topic_id'][i][2:].lstrip('0') #[2:].lstrip('0') to strip \"MB\" and leading 0s to end up with plain number\n",
    "\n",
    "        for j in range(len(resultMatrix[i][1])): #array of 1000 docs in tuples (doc_row, similarity)\n",
    "            \n",
    "            docno = df['ID'][resultMatrix[i][1][j][0]]\n",
    "            rank = j +1\n",
    "            score = resultMatrix[i][1][j][1]\n",
    "            new_line = topic_id + \" Q0 \" + str(docno) + \" \" + str(rank) + \" \" + str(score) + \" \" + tag + \"\\n\"\n",
    "            f.write(new_line)\n",
    "        \n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results('STANDARD',final_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
