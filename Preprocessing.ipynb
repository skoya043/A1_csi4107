{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "short-separation",
   "metadata": {},
   "source": [
    "# CSI 4107, Winter 2021\n",
    "## Assignment 1 - Microblog information retrieval system\n",
    "\n",
    "### Step 1: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "binding-devil",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas_read_xml as pdx\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import pairwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "extensive-patch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "      <th>title</th>\n",
       "      <th>querytime</th>\n",
       "      <th>querytweettime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Number: MB001</td>\n",
       "      <td>BBC World Service staff cuts</td>\n",
       "      <td>Tue Feb 08 12:30:27 +0000 2011</td>\n",
       "      <td>34952194402811904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Number: MB002</td>\n",
       "      <td>2022 FIFA soccer</td>\n",
       "      <td>Tue Feb 08 18:51:44 +0000 2011</td>\n",
       "      <td>35048150574039040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Number: MB003</td>\n",
       "      <td>Haiti Aristide return</td>\n",
       "      <td>Tue Feb 08 21:32:13 +0000 2011</td>\n",
       "      <td>35088534306033665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Number: MB004</td>\n",
       "      <td>Mexico drug war</td>\n",
       "      <td>Wed Feb 02 17:22:14 +0000 2011</td>\n",
       "      <td>32851298193768448</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number: MB005</td>\n",
       "      <td>NIST computer security</td>\n",
       "      <td>Fri Feb 04 17:44:09 +0000 2011</td>\n",
       "      <td>33581589627666432</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             num                         title  \\\n",
       "0  Number: MB001  BBC World Service staff cuts   \n",
       "1  Number: MB002              2022 FIFA soccer   \n",
       "2  Number: MB003         Haiti Aristide return   \n",
       "3  Number: MB004               Mexico drug war   \n",
       "4  Number: MB005        NIST computer security   \n",
       "\n",
       "                        querytime     querytweettime  \n",
       "0  Tue Feb 08 12:30:27 +0000 2011  34952194402811904  \n",
       "1  Tue Feb 08 18:51:44 +0000 2011  35048150574039040  \n",
       "2  Tue Feb 08 21:32:13 +0000 2011  35088534306033665  \n",
       "3  Wed Feb 02 17:22:14 +0000 2011  32851298193768448  \n",
       "4  Fri Feb 04 17:44:09 +0000 2011  33581589627666432  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queriesDf = pdx.read_xml(\"queries.xml\", ['queries', 'top'])\n",
    "queriesDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "virgin-heating",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿34952194402811904</td>\n",
       "      <td>Save BBC World Service from Savage Cuts http:/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34952186328784896</td>\n",
       "      <td>a lot of people always make fun about the end ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34952041415581696</td>\n",
       "      <td>ReThink Group positive in outlook: Technology ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34952018120409088</td>\n",
       "      <td>'Zombie' fund manager Phoenix appoints new CEO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34952008683229185</td>\n",
       "      <td>Latest:: Top World Releases http://globalclass...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                               text\n",
       "0  ﻿34952194402811904  Save BBC World Service from Savage Cuts http:/...\n",
       "1   34952186328784896  a lot of people always make fun about the end ...\n",
       "2   34952041415581696  ReThink Group positive in outlook: Technology ...\n",
       "3   34952018120409088  'Zombie' fund manager Phoenix appoints new CEO...\n",
       "4   34952008683229185  Latest:: Top World Releases http://globalclass..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read tweets and transform into dataframe\n",
    "data = []\n",
    "with open('Trec_microblog11.txt', 'r', encoding='utf-8', errors='replace') as infile:\n",
    "    lines = infile.readlines()\n",
    "    for i in lines:\n",
    "        data.append(i.split('\\t'))\n",
    "        \n",
    "data = np.array(data) #2d numpy array\n",
    "        \n",
    "df = pd.DataFrame({'ID': data[:, 0], 'text': data[:, 1]})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "danish-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "religious-shame",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-38-d442573762f1>:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df['text_without_stopwords'] = df['text_without_stopwords'].str.replace('\\d+', '')\n",
      "<ipython-input-38-d442573762f1>:6: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  queriesDf['text_without_stopwords'] = queriesDf['text_without_stopwords'].str.replace('\\d+', '')\n"
     ]
    }
   ],
   "source": [
    "df['text_without_stopwords'] = df['text'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in (stop)]))\n",
    "df['text_without_stopwords'] = df['text_without_stopwords'].str.replace('\\d+', '')\n",
    "df['text_without_stopwords']=df['text_without_stopwords'].apply(str.lower)\n",
    "\n",
    "queriesDf['text_without_stopwords'] = queriesDf['title'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in (stop)]))\n",
    "queriesDf['text_without_stopwords'] = queriesDf['text_without_stopwords'].str.replace('\\d+', '')\n",
    "queriesDf['text_without_stopwords'] = queriesDf['text_without_stopwords'].apply(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "parental-introduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "queriesDf['topic_id'] = queriesDf['num'].apply(lambda x : x.split(' '))\n",
    "queriesDf['topic_id'] = queriesDf['topic_id'].apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "diverse-pottery",
   "metadata": {},
   "outputs": [],
   "source": [
    "#queriesDf.head()\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "interracial-transaction",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "df['token'] = df['text_without_stopwords'].apply(tokenizer.tokenize)\n",
    "queriesDf['token'] = queriesDf['text_without_stopwords'].apply(tokenizer.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "based-guidance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# detokenize for easier query/document vectorization\n",
    "df['detoken']=df['token'].apply(lambda x: TreebankWordDetokenizer().detokenize(x))\n",
    "queriesDf['detoken']=queriesDf['token'].apply(lambda x: TreebankWordDetokenizer().detokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "vocational-product",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>num</th>\n",
       "      <th>title</th>\n",
       "      <th>querytime</th>\n",
       "      <th>querytweettime</th>\n",
       "      <th>text_without_stopwords</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>token</th>\n",
       "      <th>detoken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Number: MB001</td>\n",
       "      <td>BBC World Service staff cuts</td>\n",
       "      <td>Tue Feb 08 12:30:27 +0000 2011</td>\n",
       "      <td>34952194402811904</td>\n",
       "      <td>bbc world service staff cuts</td>\n",
       "      <td>MB001</td>\n",
       "      <td>[bbc, world, service, staff, cuts]</td>\n",
       "      <td>bbc world service staff cuts</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Number: MB002</td>\n",
       "      <td>2022 FIFA soccer</td>\n",
       "      <td>Tue Feb 08 18:51:44 +0000 2011</td>\n",
       "      <td>35048150574039040</td>\n",
       "      <td>fifa soccer</td>\n",
       "      <td>MB002</td>\n",
       "      <td>[fifa, soccer]</td>\n",
       "      <td>fifa soccer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Number: MB003</td>\n",
       "      <td>Haiti Aristide return</td>\n",
       "      <td>Tue Feb 08 21:32:13 +0000 2011</td>\n",
       "      <td>35088534306033665</td>\n",
       "      <td>haiti aristide return</td>\n",
       "      <td>MB003</td>\n",
       "      <td>[haiti, aristide, return]</td>\n",
       "      <td>haiti aristide return</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Number: MB004</td>\n",
       "      <td>Mexico drug war</td>\n",
       "      <td>Wed Feb 02 17:22:14 +0000 2011</td>\n",
       "      <td>32851298193768448</td>\n",
       "      <td>mexico drug war</td>\n",
       "      <td>MB004</td>\n",
       "      <td>[mexico, drug, war]</td>\n",
       "      <td>mexico drug war</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Number: MB005</td>\n",
       "      <td>NIST computer security</td>\n",
       "      <td>Fri Feb 04 17:44:09 +0000 2011</td>\n",
       "      <td>33581589627666432</td>\n",
       "      <td>nist computer security</td>\n",
       "      <td>MB005</td>\n",
       "      <td>[nist, computer, security]</td>\n",
       "      <td>nist computer security</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             num                         title  \\\n",
       "0  Number: MB001  BBC World Service staff cuts   \n",
       "1  Number: MB002              2022 FIFA soccer   \n",
       "2  Number: MB003         Haiti Aristide return   \n",
       "3  Number: MB004               Mexico drug war   \n",
       "4  Number: MB005        NIST computer security   \n",
       "\n",
       "                        querytime     querytweettime  \\\n",
       "0  Tue Feb 08 12:30:27 +0000 2011  34952194402811904   \n",
       "1  Tue Feb 08 18:51:44 +0000 2011  35048150574039040   \n",
       "2  Tue Feb 08 21:32:13 +0000 2011  35088534306033665   \n",
       "3  Wed Feb 02 17:22:14 +0000 2011  32851298193768448   \n",
       "4  Fri Feb 04 17:44:09 +0000 2011  33581589627666432   \n",
       "\n",
       "         text_without_stopwords topic_id                               token  \\\n",
       "0  bbc world service staff cuts    MB001  [bbc, world, service, staff, cuts]   \n",
       "1                   fifa soccer    MB002                      [fifa, soccer]   \n",
       "2         haiti aristide return    MB003           [haiti, aristide, return]   \n",
       "3               mexico drug war    MB004                 [mexico, drug, war]   \n",
       "4        nist computer security    MB005          [nist, computer, security]   \n",
       "\n",
       "                        detoken  \n",
       "0  bbc world service staff cuts  \n",
       "1                   fifa soccer  \n",
       "2         haiti aristide return  \n",
       "3               mexico drug war  \n",
       "4        nist computer security  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queriesDf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "loaded-brief",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>text</th>\n",
       "      <th>text_without_stopwords</th>\n",
       "      <th>token</th>\n",
       "      <th>detoken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>﻿34952194402811904</td>\n",
       "      <td>Save BBC World Service from Savage Cuts http:/...</td>\n",
       "      <td>save bbc world service savage cuts http://www....</td>\n",
       "      <td>[save, bbc, world, service, savage, cuts, http...</td>\n",
       "      <td>save bbc world service savage cuts http www pe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>34952186328784896</td>\n",
       "      <td>a lot of people always make fun about the end ...</td>\n",
       "      <td>lot people always make fun end world question ...</td>\n",
       "      <td>[lot, people, always, make, fun, end, world, q...</td>\n",
       "      <td>lot people always make fun end world question ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34952041415581696</td>\n",
       "      <td>ReThink Group positive in outlook: Technology ...</td>\n",
       "      <td>rethink group positive outlook: technology sta...</td>\n",
       "      <td>[rethink, group, positive, outlook, technology...</td>\n",
       "      <td>rethink group positive outlook technology staf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34952018120409088</td>\n",
       "      <td>'Zombie' fund manager Phoenix appoints new CEO...</td>\n",
       "      <td>'zombie' fund manager phoenix appoints new ceo...</td>\n",
       "      <td>[zombie, fund, manager, phoenix, appoints, new...</td>\n",
       "      <td>zombie fund manager phoenix appoints new ceo p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34952008683229185</td>\n",
       "      <td>Latest:: Top World Releases http://globalclass...</td>\n",
       "      <td>latest:: top world releases http://globalclass...</td>\n",
       "      <td>[latest, top, world, releases, http, globalcla...</td>\n",
       "      <td>latest top world releases http globalclassifie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                               text  \\\n",
       "0  ﻿34952194402811904  Save BBC World Service from Savage Cuts http:/...   \n",
       "1   34952186328784896  a lot of people always make fun about the end ...   \n",
       "2   34952041415581696  ReThink Group positive in outlook: Technology ...   \n",
       "3   34952018120409088  'Zombie' fund manager Phoenix appoints new CEO...   \n",
       "4   34952008683229185  Latest:: Top World Releases http://globalclass...   \n",
       "\n",
       "                              text_without_stopwords  \\\n",
       "0  save bbc world service savage cuts http://www....   \n",
       "1  lot people always make fun end world question ...   \n",
       "2  rethink group positive outlook: technology sta...   \n",
       "3  'zombie' fund manager phoenix appoints new ceo...   \n",
       "4  latest:: top world releases http://globalclass...   \n",
       "\n",
       "                                               token  \\\n",
       "0  [save, bbc, world, service, savage, cuts, http...   \n",
       "1  [lot, people, always, make, fun, end, world, q...   \n",
       "2  [rethink, group, positive, outlook, technology...   \n",
       "3  [zombie, fund, manager, phoenix, appoints, new...   \n",
       "4  [latest, top, world, releases, http, globalcla...   \n",
       "\n",
       "                                             detoken  \n",
       "0  save bbc world service savage cuts http www pe...  \n",
       "1  lot people always make fun end world question ...  \n",
       "2  rethink group positive outlook technology staf...  \n",
       "3  zombie fund manager phoenix appoints new ceo p...  \n",
       "4  latest top world releases http globalclassifie...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "described-fusion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['australia', 's', 'greatest', 'olympian', 'ian', 'thorpe', 'expected', 'reportedly', 'announce', 'comeback', 'international', 'swim', 'http', 'tf', 'to', 'fmg']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#df['token'][26]#test to see if numbers have been removed\n",
    "print(df['token'][12898])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becoming-music",
   "metadata": {},
   "source": [
    "### Step 2: Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "north-vietnamese",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# key, [(doc, count)]\n",
    "# key [doc]\n",
    "\n",
    "\n",
    "def create_index (data):\n",
    "        index = defaultdict(list)\n",
    "        count=0\n",
    "        for i, tokens in enumerate(data):\n",
    "            for token in tokens:\n",
    "                index[token].append(i)\n",
    "                #index[save].append(doc)\n",
    "        return index\n",
    "\n",
    "\n",
    "\n",
    "def count_occurrences(arr):\n",
    "    return list(Counter(arr).items())\n",
    "\n",
    "#    for d in dictionary:\n",
    "#        return list(Counter(dictionary[d]).items())\n",
    "\n",
    "def final_index(index):\n",
    "    final=defaultdict(list)\n",
    "    for key in index.keys():\n",
    "        final[key] = count_occurrences(index[key])\n",
    "    return final\n",
    "\n",
    "# each token (save, bbc) as your dict key\n",
    "# append the count_occurrences based on index\n",
    "#final_index creates the final hash table for inverted index. {word: [(doc#, number of occurrences in that document)]}\n",
    "# doc# in this case is the line number of the table, not the message ID.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fuzzy-teens",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87685\n"
     ]
    }
   ],
   "source": [
    "test=create_index(df['token'])\n",
    "\n",
    "test2=count_occurrences([0, 1, 44, 44, 443])\n",
    "#test2\n",
    "test3=final_index(test)\n",
    "#test3\n",
    "\n",
    "print(len(test3.keys()))\n",
    "# print(test[\"thorpe\"])\n",
    "# for row_num in test[\"thorpe\"]:\n",
    "#     print(df['detoken'][row_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "different-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(list(test3.keys())[0:100]) # get a sample of 100 tokens from vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-knitting",
   "metadata": {},
   "source": [
    "## Step 3: Retrieval and Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "enclosed-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query: list of tokens for a query\n",
    "# docs: inverted hash index with key:value of token:[(doc_num,num_occurances,...)]\n",
    "def related_documents_to_query(query, docs):\n",
    "    related_docs = []\n",
    "    for token in query:\n",
    "        if token in docs:\n",
    "            related_docs+=docs[token]\n",
    "\n",
    "    nums_only = []\n",
    "    for x in related_docs:\n",
    "        nums_only.append(x[0])\n",
    "\n",
    "    return set(nums_only)\n",
    "                    \n",
    "    \n",
    "##create td-idf matrix\n",
    "# creating tf-idfs matrix\n",
    "corpus = df['detoken']\n",
    "# use X[doc_id, term_id] to get the td-idf for the term and document.\n",
    "docVectorizer = TfidfVectorizer()\n",
    "docMatrix = docVectorizer.fit_transform(corpus)\n",
    "docVocabulary = docVectorizer.vocabulary_\n",
    "#docMatrix.toarray()\n",
    "\n",
    "\n",
    "#find cosine similarity\n",
    "#returns an array of (doc_num, similarity)\n",
    "def similarity_cosine(query, docs):\n",
    "    sim_arr = []\n",
    "    qvec = docVectorizer.transform([query])\n",
    "    \n",
    "    for doc in docs: \n",
    "        similarity = pairwise.cosine_similarity(doc[1], qvec)\n",
    "        sim_arr.append((doc[0], similarity[0][0]))\n",
    "    return sim_arr \n",
    "\n",
    "\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(docMatrix.toarray())\n",
    "\n",
    "##for each query, get set of related docs (doc #, num of occurences)\n",
    "final_results = []\n",
    "for index, q_row in queriesDf.iterrows():\n",
    "    \n",
    "    related_docs = related_documents_to_query(q_row['token'], test3)\n",
    "    \n",
    "    related_docs_matrix = []\n",
    "    #get vectors of related docs\n",
    "    for doc_num in related_docs:\n",
    "        related_docs_matrix.append((doc_num, docMatrix[doc_num]) )\n",
    "        \n",
    "    # get similarities\n",
    "    sim_arr = similarity_cosine(q_row['detoken'], related_docs_matrix)\n",
    "    ranked = sorted(sim_arr, key = lambda x : x[1], reverse = True)\n",
    "    \n",
    "    # rank top 1000\n",
    "    top_1000 = ranked[0:1000]\n",
    "    #print(top_1000)\n",
    "    #print(df['detoken'][top_1000[0][0]])\n",
    "    \n",
    "    # add to final results array\n",
    "    final_results.append( (q_row['num'], top_1000) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-stevens",
   "metadata": {},
   "source": [
    "## Step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "loving-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample line: topic_id Q0 docno rank score tag\n",
    "# topic_id: queriesDf['topic_id'][i]\n",
    "# docno: df['ID'][resultsMatrix[i][j][0]]\n",
    "# rank: j from resultsMatrix[i], +1 because highest rank is 1 not 0\n",
    "# score: resultsMatrix[i][j][1]\n",
    "# tag: passed in at function call\n",
    "\n",
    "def write_results(tag, resultMatrix):\n",
    "    \n",
    "    f = open(\"results1.txt\", \"w+\", encoding=\"utf-8\")\n",
    "    \n",
    "    for i in range(len(resultMatrix)):\n",
    "        topic_id = queriesDf['topic_id'][i][2:].lstrip('0') #[2:].lstrip('0') to strip \"MB\" and leading 0s to end up with plain number\n",
    "\n",
    "        for j in range(len(resultMatrix[i][1])): #array of 1000 docs in tuples (doc_row, similarity)\n",
    "            \n",
    "            docno = df['ID'][resultMatrix[i][1][j][0]]\n",
    "            rank = j +1\n",
    "            score = resultMatrix[i][1][j][1]\n",
    "            new_line = topic_id + \" Q0 \" + str(docno) + \" \" + str(rank) + \" \" + str(score) + \" \" + tag + \"\\n\"\n",
    "            f.write(new_line)\n",
    "        \n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-windows",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_results('STANDARD',final_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-struggle",
   "metadata": {},
   "source": [
    "### Step 3: Retrieval and Ranking\n",
    "#### 3.1. Vectorization of the corpus and extraction of the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-uncertainty",
   "metadata": {},
   "source": [
    "#### 3.2. Function to vectorize a query using the document vocabulary and queries vectorization\n",
    "\n",
    "The function below returns an array of len = # queries. Each element in the array has a len = # of terms in the corpus and tf-idf values for each term in the query only terms that are both in the query and the corpus will have tf-idf values since the vocabulary of the corpus is being used for the vectorization of the query. In the assignment, we have a total of 50953 terms that will be used as part of the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "modern-literacy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for query processing\n",
    "def transform_query (q, voc):\n",
    "    queryVectorizer = TfidfVectorizer(vocabulary = voc)\n",
    "    qMatrix = queryVectorizer.fit_transform(q)\n",
    "    qMatrixArr = qMatrix.toarray()\n",
    "    return qMatrixArr[0]\n",
    "\n",
    "#sample_query = 'BBC World Service staff cuts'\n",
    "#sample_query_transformation = transform_query([sample_query], docVocabulary)\n",
    "#print(len(sample_query_transformation)) #should return 50953 which is the total number of terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-specific",
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries vectorization\n",
    "#queriesDf['title'][0]\n",
    "\n",
    "queriesVectArr = []\n",
    "\n",
    "for i in range (len(queriesDf['title'])):\n",
    "    currQuery = queriesDf['title'][i]\n",
    "    currQueryVect = transform_query([currQuery], docVocabulary)\n",
    "    queriesVectArr.append(currQueryVect)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-elevation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(queriesVectArr[0])) # this should return 50953"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-money",
   "metadata": {},
   "outputs": [],
   "source": [
    "testQueriesMatrix = transform_query([queriesDf['title'][0]], docVocabulary)\n",
    "similarityMatrix = pairwise.cosine_similarity(queriesVectArr, docMatrixArr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metropolitan-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(similarityMatrix[0])\n",
    "# similarityMatrix[0][0]\n",
    "# type(similarityMatrix)  # numpy.ndarray"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
